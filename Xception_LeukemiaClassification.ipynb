{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xception-LeukemiaClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6J-Iz0Kr_as"
      },
      "source": [
        "# Modello realizzato da zero\n",
        "\n",
        "## Inizializzazione librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEwdMWXw-AOP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "import os\n",
        "import tqdm\n",
        "import skimage.io\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from skimage.io import imread, imshow, imsave\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Instantiating the model for loading the weights and biases and preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8dm9GiM-aXN"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6_BTe64-cxH",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5e5f73e4-d8df-496f-fb44-1551c3eebb16"
      },
      "source": [
        "from google.colab import files \n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3dfdb64e-e772-4802-8658-687036c79b99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3dfdb64e-e772-4802-8658-687036c79b99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"martinacavallucci\",\"key\":\"1813c6e53fa0a380c7ec0e80fefbebdd\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO2ZMq8w-eJb"
      },
      "source": [
        "! mkdir ~/.kaggle \n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxJ3KTU-fqI"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaOxicD-sQpa"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_QvAWfy-iup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e27ad2-bb3f-40e7-b66c-b415190b9da7"
      },
      "source": [
        "! kaggle datasets download -d andrewmvd/leukemia-classification"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading leukemia-classification.zip to /content\n",
            " 99% 862M/867M [00:08<00:00, 87.3MB/s]\n",
            "100% 867M/867M [00:08<00:00, 107MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbFOqiA_-k--"
      },
      "source": [
        "! unzip leukemia-classification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYndKCShs0Uv"
      },
      "source": [
        "## Caricamento nomi immagini dalle cartelle del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjp_ntlIO8rv"
      },
      "source": [
        "# # delete_test_file = glob.glob('/content/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data/*.bmp')\n",
        "# # for file in delete_test_file:\n",
        "# #   os.remove(file)\n",
        "# !ls\n",
        "\n",
        "# ! unzip C-NMC_test_prelim_phase_data.zip -d /content/C-NMC_Leukemia/validation_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mnkt6R9-mz_"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "if os.path.exists(\"./leukemia-classification.zip\"):\n",
        "  os.remove(\"./leukemia-classification.zip\")\n",
        "# Reading Data\n",
        "# elimino file non utilizzabili\n",
        "delete_test_file = glob.glob('./C-NMC_Leukemia/testing_data/C-NMC_test_final_phase_data/*.bmp')\n",
        "for file in delete_test_file:\n",
        "  os.remove(file)\n",
        "\n",
        "train_dataset_0_all = glob.glob('./C-NMC_Leukemia/training_data/fold_0/all/*.bmp')\n",
        "train_dataset_0_hem = glob.glob('./C-NMC_Leukemia/training_data/fold_0/hem/*.bmp')\n",
        "train_dataset_1_all = glob.glob('./C-NMC_Leukemia/training_data/fold_1/all/*.bmp')\n",
        "train_dataset_1_hem = glob.glob('./C-NMC_Leukemia/training_data/fold_1/hem/*.bmp')\n",
        "train_dataset_2_all = glob.glob('./C-NMC_Leukemia/training_data/fold_2/all/*.bmp')\n",
        "train_dataset_2_hem = glob.glob('./C-NMC_Leukemia/training_data/fold_2/hem/*.bmp')\n",
        "\n",
        "# test_dataset  = glob.glob('./C-NMC_Leukemia/testing_data/C-NMC_test_final_phase_data/*.bmp')\n",
        "# valid_dataset = glob.glob('./C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data/*.bmp')\n",
        "\n",
        "valid_data = pd.read_csv('./C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data_labels.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mSJN4sas7p4"
      },
      "source": [
        "Definizione proprietà delle immagini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVVD4Z9J9FON"
      },
      "source": [
        "IMAGE_WIDTH = 450\n",
        "IMAGE_HEIGHT = 450\n",
        "\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 3"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOgKBLLs-9o"
      },
      "source": [
        "mergin immagini delle differenti classi e delle loro label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A9jyf-d49HL"
      },
      "source": [
        "# RGB_SCALE = 255\n",
        "# CMYK_SCALE = 255\n",
        "\n",
        "\n",
        "# def rgb_to_cmyk(img):\n",
        "#     R = img[:,:,0]\n",
        "#     G = img[:,:,1]\n",
        "#     B = img[:,:,2]\n",
        "\n",
        "#     c = 1 - R / RGB_SCALE\n",
        "#     m = 1 - G / RGB_SCALE\n",
        "#     y = 1 - B / RGB_SCALE\n",
        "#     min_cmy = np.minimum(c, np.minimum(m, y))\n",
        "#     c = ((c - min_cmy) / (1 - min_cmy)) * CMYK_SCALE\n",
        "#     m = ((m - min_cmy) / (1 - min_cmy)) * CMYK_SCALE\n",
        "#     y = ((y - min_cmy) / (1 - min_cmy)) * CMYK_SCALE\n",
        "#     result = np.zeros(img.shape, dtype=\"uint8\")\n",
        "#     result[:,:,0] = c.astype(int)\n",
        "#     result[:,:,1] = m.astype(int)\n",
        "#     result[:,:,2] = y.astype(int)\n",
        "#     return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHodGdpdVi1-"
      },
      "source": [
        "dataset_all = train_dataset_0_all + train_dataset_1_all + train_dataset_2_all\n",
        "dataset_hem = train_dataset_0_hem + train_dataset_1_hem + train_dataset_2_hem \n",
        "\n",
        "\n",
        "label_all =  [\"1\" for x in range(len(dataset_all))] \n",
        "label_hem = [\"0\" for x in range(len(dataset_hem))] \n",
        "\n",
        "dataset = dataset_all + dataset_hem\n",
        "label = label_all + label_hem"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toFkMrOj5gqf"
      },
      "source": [
        "# with tensorflow.device('/device:GPU:0'):\n",
        "#   for filename in tqdm(dataset):\n",
        "#     image = imread(filename)\n",
        "#     img = rgb_to_cmyk(image)\n",
        "#     imsave(filename, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaF-zzcvXPk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269f1c8c-6f05-4db8-a6ad-6fd9501455f4"
      },
      "source": [
        "print(\"# image All: \" + str(len(dataset_all)))\n",
        "print(\"# image Hem: \" + str(len(dataset_hem)))\n",
        "print(\"# image: \" + str(len(dataset)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# image All: 7272\n",
            "# image Hem: 3389\n",
            "# image: 10661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PP_JI1PtHf-"
      },
      "source": [
        "## Data Augmetation\n",
        "In questa fase verà eseguato della data augmetation alle immagini per ottenre un dataset che comprenda 10000 immagini per ogni classe del problema.\n",
        "\n",
        "### Campionamento delle immagini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crrCI6IeaXg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0a1b06-16f8-4e22-8a8d-b58958d5c0c5"
      },
      "source": [
        "\n",
        "NUM_AUGMETATION = 2\n",
        "\n",
        "augmetedImage_all = random.sample(dataset_all, (10000 - len(dataset_all)))\n",
        "print(\"# image To augment (All): \" + str(len(augmetedImage_all)))\n",
        "\n",
        "augmetedImage_hem = random.sample(dataset_hem + dataset_hem, (10000 - len(dataset_hem)))\n",
        "print(\"# image To augment (hem): \" + str(len(augmetedImage_hem)))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# image To augment (All): 2728\n",
            "# image To augment (hem): 6611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98-61As2tSKW"
      },
      "source": [
        "### Esecuzione Data augmetation\n",
        "\n",
        "Le operazioni effetuate sulla immagine sono:\n",
        "\n",
        "\n",
        "1.   Flip casuale in una delle 2 direzioni (verticale / orizontale)\n",
        "2.   Rotazione casuale\n",
        "3.   Cropping casuale dell'immgine\n",
        "4.   modifica casuale del contrasto dell'immagine\n",
        "5.   Resizing per riportarel e immagini croppate alla dimensione originale\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Is8Xa7ct_9"
      },
      "source": [
        "data_augmentation = tensorflow.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", seed=42),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2,seed=42)\n",
        "])\n",
        "\n",
        "data_augmentation_whit_crop = tensorflow.keras.Sequential([\n",
        "  data_augmentation,\n",
        "  layers.experimental.preprocessing.CenterCrop(400,400),\n",
        "  layers.experimental.preprocessing.Resizing(450,450)\n",
        "])\n",
        "\n",
        "augmeted_all = []\n",
        "out_folder = \"./C-NMC_Leukemia/training_data/fold_a/all/\"\n",
        "if not os.path.exists(out_folder):\n",
        "  out_folder_a = \"./C-NMC_Leukemia/training_data/fold_a\"\n",
        "  os.mkdir(out_folder_a, mode=777)\n",
        "  os.mkdir(out_folder, mode=777)\n",
        "\n",
        "with tensorflow.device('/device:GPU:0'):\n",
        "  for j in tqdm(range(0, len(augmetedImage_all))):\n",
        "    image = imread(augmetedImage_all[j])\n",
        "    image2 = tensorflow.expand_dims(image, 0)\n",
        "    aug_img =  data_augmentation_whit_crop(image2) if j < 1000 else data_augmentation(image2)\n",
        "    imsave(out_folder + str(j) + '_' + os.path.basename(augmetedImage_all[j]), aug_img[0].numpy())\n",
        "\n",
        "augmeted_hem = []\n",
        "out_folder = \"./C-NMC_Leukemia/training_data/fold_a/hem/\"\n",
        "if not os.path.exists(out_folder):\n",
        "  os.mkdir(out_folder, mode=777)\n",
        "  \n",
        "with tensorflow.device('/device:GPU:0'):\n",
        "  for j in tqdm(range(0, len(augmetedImage_hem))):\n",
        "    image = imread(augmetedImage_hem[j])\n",
        "    image2 = tensorflow.expand_dims(image, 0)\n",
        "    aug_img = data_augmentation_whit_crop(image2) if j < 1000 else data_augmentation(image2)\n",
        "    imsave(out_folder + str(j) + '_' + os.path.basename(augmetedImage_hem[j]), aug_img[0].numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoudFjJBvCx1"
      },
      "source": [
        "Aggiungo al dataset e alle label le immagini aumentate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4JCRzb5ROj"
      },
      "source": [
        "train_dataset_a_all = glob.glob('./C-NMC_Leukemia/training_data/fold_a/all/*.bmp')\n",
        "train_dataset_a_hem = glob.glob('./C-NMC_Leukemia/training_data/fold_a/hem/*.bmp')\n",
        "\n",
        "dataset = dataset + train_dataset_a_all + train_dataset_a_hem\n",
        "\n",
        "label_a_all = [\"1\" for x in range(len(train_dataset_a_all))] \n",
        "label_a_hem = [\"0\" for x in range(len(train_dataset_a_hem))] \n",
        "\n",
        "label = label+ label_a_all + label_a_hem"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRBZcKX_vaki"
      },
      "source": [
        "## Preprazione dati e modello\n",
        "In questa fase vengono preparati i dati per entrare nel modello e viene realizzato il vero e proprio modello\n",
        "\n",
        "### Preparazione dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cIa9rub6R_N"
      },
      "source": [
        "X_val, Y_val = shuffle(dataset, label, random_state = 42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSF8yA6N65pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d82c5f-6d92-4215-a0e2-19693c5805d4"
      },
      "source": [
        "d = {'filename':X_val, 'category':Y_val}\n",
        "enteireData = pd.DataFrame(data=d)\n",
        "\n",
        "train_df, validate_df = train_test_split(enteireData, test_size = 0.20, random_state = 42)\n",
        "train_df = train_df.reset_index(drop = True)\n",
        "validate_df = validate_df.reset_index(drop = True)\n",
        "print(train_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                filename category\n",
            "0      ./C-NMC_Leukemia/training_data/fold_a/hem/2497...        0\n",
            "1      ./C-NMC_Leukemia/training_data/fold_2/all/UID_...        1\n",
            "2      ./C-NMC_Leukemia/training_data/fold_1/all/UID_...        1\n",
            "3      ./C-NMC_Leukemia/training_data/fold_2/all/UID_...        1\n",
            "4      ./C-NMC_Leukemia/training_data/fold_1/hem/UID_...        0\n",
            "...                                                  ...      ...\n",
            "15995  ./C-NMC_Leukemia/training_data/fold_a/hem/5748...        0\n",
            "15996  ./C-NMC_Leukemia/training_data/fold_0/hem/UID_...        0\n",
            "15997  ./C-NMC_Leukemia/training_data/fold_2/hem/UID_...        0\n",
            "15998  ./C-NMC_Leukemia/training_data/fold_a/hem/3656...        0\n",
            "15999  ./C-NMC_Leukemia/training_data/fold_a/hem/1064...        0\n",
            "\n",
            "[16000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBcpHEJW8HlR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ae425e18-97dd-4e60-95d0-14d750b91773"
      },
      "source": [
        "train_df['category'].value_counts().plot.bar()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff232a5ead0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diNtVE-v8O7u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8d9c9767-590d-4178-e2e5-14852c9d488d"
      },
      "source": [
        "validate_df['category'].value_counts().plot.bar()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd7ddd71390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjYfzIcf9Ml8"
      },
      "source": [
        "batch_size = 128"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUSz3C2Av1oI"
      },
      "source": [
        "### Initializzazione generatori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0sk72C38Vpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4531821c-db71-4c36-a7e2-595ab1849b95"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 30,\n",
        "    rescale = 1 / 255.0,\n",
        "    shear_range = False,\n",
        "    zoom_range = False,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df, \n",
        "    \"./\",\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category',\n",
        "    target_size = IMAGE_SIZE,\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16000 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quw28w7X9pF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96716490-901e-4f90-e302-5d166ce5bbe7"
      },
      "source": [
        "validation_datagen = ImageDataGenerator(\n",
        "    rotation_range = 30,\n",
        "    rescale = 1 / 255.0,\n",
        "    shear_range = False,\n",
        "    zoom_range = False,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_dataframe(\n",
        "    validate_df,\n",
        "    \"./\",\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category',\n",
        "    target_size = IMAGE_SIZE,\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4000 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7b08Qyz9xse"
      },
      "source": [
        "# example_df = train_df.sample(n = 15).reset_index(drop = True)\n",
        "\n",
        "# example_generator = train_datagen.flow_from_dataframe(\n",
        "#     example_df,\n",
        "#     \"./\",\n",
        "#     x_col = 'filename',\n",
        "#     y_col = 'category',\n",
        "#     target_size = IMAGE_SIZE,\n",
        "#     class_mode = 'categorical',\n",
        "#     batch_size = batch_size\n",
        "# )\n",
        "\n",
        "# plt.figure(figsize = (12, 12))\n",
        "\n",
        "# for i in range(0, 64):\n",
        "#     plt.subplot(8, 8, i + 1)\n",
        "    \n",
        "#     for X_batch, Y_batch in example_generator:\n",
        "#         image = X_batch[0]\n",
        "#         plt.imshow(image)\n",
        "#         break\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g_rOtqD-IPC"
      },
      "source": [
        "total_train = train_df.shape[0]\n",
        "total_validate = validate_df.shape[0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBPMYVfewACV"
      },
      "source": [
        "### Preparazione del modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJp3JRphwEAg"
      },
      "source": [
        "Definizione delle callback e parametri per il fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABQy7hty4RwY"
      },
      "source": [
        "earlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 10, verbose = 1)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor = \"val_accuracy\", patience = 2, verbose = 1, factor = 0.5, min_lr = 0.00001)\n",
        "checkpoint = ModelCheckpoint(\"Xception_best_model.h5\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True, mode = \"auto\")\n",
        "callbacks = [earlystop, learning_rate_reduction, checkpoint]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qH4JO6Iu9VO",
        "outputId": "9320608a-56e1-4fd7-c247-6de3d8ac0095"
      },
      "source": [
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "base_model = Xception(include_top=False, weights='imagenet', input_shape=(450,450,3))\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tensorflow.keras.Input(shape=(450, 450, 3))\n",
        "# We make sure that the base_model is running in inference mode here,\n",
        "# by passing `training=False`. This is important for fine-tuning, as you will\n",
        "# learn in a few paragraphs.\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = tensorflow.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier with a single unit (binary classification)\n",
        "\n",
        "final_layer = tensorflow.keras.layers.Dense(2, activation = \"softmax\")(x)\n",
        "model = tensorflow.keras.Model(inputs, final_layer)\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    metrics = ['accuracy'], \n",
        "    optimizer = 'adam'\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 450, 450, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 14, 14, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 20,865,578\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u53O1OK55TZK",
        "outputId": "674c83f7-2622-4ac3-c9c3-7838f5787900"
      },
      "source": [
        "epochs = 5\n",
        "with tensorflow.device('/device:GPU:0'):\n",
        "  model.fit(\n",
        "      train_generator, \n",
        "      epochs = epochs,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps = total_validate // batch_size,\n",
        "      steps_per_epoch = total_train // batch_size,\n",
        "      callbacks = callbacks\n",
        "  )\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 988s 8s/step - loss: 0.5281 - accuracy: 0.7441 - val_loss: 0.4862 - val_accuracy: 0.7724\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.77243, saving model to Xception_best_model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n",
            "125/125 [==============================] - 968s 8s/step - loss: 0.4725 - accuracy: 0.7795 - val_loss: 0.4668 - val_accuracy: 0.7848\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.77243 to 0.78478, saving model to Xception_best_model.h5\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 964s 8s/step - loss: 0.4561 - accuracy: 0.7886 - val_loss: 0.4507 - val_accuracy: 0.7921\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.78478 to 0.79209, saving model to Xception_best_model.h5\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 966s 8s/step - loss: 0.4483 - accuracy: 0.7930 - val_loss: 0.4515 - val_accuracy: 0.7886\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.79209\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 966s 8s/step - loss: 0.4445 - accuracy: 0.7915 - val_loss: 0.4535 - val_accuracy: 0.7951\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.79209 to 0.79511, saving model to Xception_best_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54VaohO31q3D"
      },
      "source": [
        "\n",
        "\n",
        "model.load_weights(\"Xception_best_model.h5\")\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8yI94swlCoP"
      },
      "source": [
        "earlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 10, verbose = 1)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor = \"val_accuracy\", patience = 2, verbose = 1, factor = 0.5, min_lr = 0.00001)\n",
        "checkpoint = ModelCheckpoint(\"Xception_best_model2.h5\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True, mode = \"auto\")\n",
        "callbacks2 = [earlystop, learning_rate_reduction, checkpoint]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_cvzT1iyqi",
        "outputId": "305a84d1-aeee-4b80-b890-1ded640ccd85"
      },
      "source": [
        "epochs = 10\n",
        "model.fit(\n",
        "    train_generator, \n",
        "    epochs = epochs,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = total_validate // batch_size,\n",
        "    steps_per_epoch = total_train // batch_size,\n",
        "    callbacks = callbacks2\n",
        " )"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 993s 8s/step - loss: 0.4370 - accuracy: 0.8022 - val_loss: 0.4391 - val_accuracy: 0.8014\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.80141, saving model to Xception_best_model2.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "125/125 [==============================] - 999s 8s/step - loss: 0.4346 - accuracy: 0.7996 - val_loss: 0.4359 - val_accuracy: 0.8009\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.80141\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 993s 8s/step - loss: 0.4257 - accuracy: 0.8051 - val_loss: 0.4289 - val_accuracy: 0.8082\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.80141 to 0.80822, saving model to Xception_best_model2.h5\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 991s 8s/step - loss: 0.4254 - accuracy: 0.8082 - val_loss: 0.4222 - val_accuracy: 0.8082\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.80822\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 984s 8s/step - loss: 0.4224 - accuracy: 0.8083 - val_loss: 0.4234 - val_accuracy: 0.8062\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.80822\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 973s 8s/step - loss: 0.4220 - accuracy: 0.8102 - val_loss: 0.4176 - val_accuracy: 0.8122\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.80822 to 0.81225, saving model to Xception_best_model2.h5\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 977s 8s/step - loss: 0.4187 - accuracy: 0.8109 - val_loss: 0.4226 - val_accuracy: 0.8085\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.81225\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 979s 8s/step - loss: 0.4193 - accuracy: 0.8089 - val_loss: 0.4254 - val_accuracy: 0.8022\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.81225\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 984s 8s/step - loss: 0.4161 - accuracy: 0.8110 - val_loss: 0.4213 - val_accuracy: 0.8135\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.81225 to 0.81351, saving model to Xception_best_model2.h5\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 988s 8s/step - loss: 0.4154 - accuracy: 0.8131 - val_loss: 0.4149 - val_accuracy: 0.8095\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.81351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa42d929810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nkt1-Il4EU"
      },
      "source": [
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = tensorflow.keras.models.load_model(\"Xception_best_model2.h5\")\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63MYLVUDLfTB",
        "outputId": "2230ce5a-ef0c-43b7-d3b0-e1f262eadd85"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 450, 450, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 14, 14, 2048)      20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 20,865,578\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsW5xIQOK6Pd"
      },
      "source": [
        "for i,layer in enumerate(new_model.layers):\n",
        "  if i < 115:\n",
        "    layers.trainable = False\n",
        "  else:\n",
        "    layer.trainable = True"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YBWuAogmbJL"
      },
      "source": [
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    metrics = ['accuracy'], \n",
        "    optimizer = 'adam'\n",
        ")\n",
        "earlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 10, verbose = 1)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor = \"val_accuracy\", patience = 2, verbose = 1, factor = 0.5, min_lr = 0.00001)\n",
        "checkpoint =  ModelCheckpoint(\"final_Xception.h5\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True, mode = \"auto\")\n",
        "callbacks3 = [earlystop, learning_rate_reduction, checkpoint]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMFKT3ujMSrh"
      },
      "source": [
        "epochs = 10"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WAzBV-GmlY9",
        "outputId": "873ee653-17c2-4886-b76a-0ce895f79b8a"
      },
      "source": [
        "history = model.fit(\n",
        "    train_generator, \n",
        "    epochs = epochs,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = total_validate // batch_size,\n",
        "    steps_per_epoch = total_train // batch_size,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 34/125 [=======>......................] - ETA: 9:43 - loss: 0.4164 - accuracy: 0.8174"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2MOkBG5-bnG"
      },
      "source": [
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Model accuracy\")\n",
        "plt.ylabel(\"%\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train accuracy\",\"validation accuracy\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s18tFLCguqvO"
      },
      "source": [
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Evaluation\")\n",
        "plt.ylabel(\"\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"validation accuracy\",\"validation loss\"], loc=\"upper left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D2BVp8LJnc3"
      },
      "source": [
        "inp_test_path = \"/content/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data/\"\n",
        "\n",
        "valid_data_file = [inp_test_path + x for x in valid_data[\"new_names\"]]\n",
        "valid_data_label = [str(x) for x in valid_data[\"labels\"]]\n",
        "# with tensorflow.device('/device:GPU:0'):\n",
        "#   for filename in tqdm(valid_data_file):\n",
        "#     image = imread(filename)\n",
        "#     img = rgb_to_cmyk(image)\n",
        "#     imsave(filename, img)\n",
        "\n",
        "test_d = {'filename':valid_data_file, 'category':valid_data_label}\n",
        "enteireData_test = pd.DataFrame(data=test_d)\n",
        "\n",
        "print(enteireData_test[enteireData_test['category'] == '1'].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HdrAKj-FMi"
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1 / 255.0,\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    enteireData_test,\n",
        "    \"./\",\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category',\n",
        "    target_size = IMAGE_SIZE,\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHP1hUmT-nUD"
      },
      "source": [
        "Y_pred = model.predict_generator(test_generator, enteireData_test.shape[0] // batch_size+1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSMHbItuBBD_"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "#y_pred_keras = keras_model.predict(X_test).ravel()\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(test_generator.classes, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXQ_lWFRJQXJ"
      },
      "source": [
        "print('Confusion Matrix')\n",
        "print(enteireData_test.shape[0])\n",
        "print(confusion_matrix(test_generator.classes, y_pred))\n",
        "\n",
        "print('Classification Report')\n",
        "target_names = ['Hem', 'All']\n",
        "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm1T-3pm-cF4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title = 'Confusion matrix',\n",
        "                          cmap = None,\n",
        "                          normalize = True):\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Reds')\n",
        "\n",
        "    plt.figure(figsize = (8, 6))\n",
        "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation = 45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment = \"center\",\n",
        "                     color = \"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment = \"center\",\n",
        "                     color = \"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy = {:0.4f}; misclass = {:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()\n",
        "\n",
        "img1 = plot_confusion_matrix(cm = np.array([[183, 465],[317, 902]]), \n",
        "                            normalize = False,\n",
        "                            target_names = ['Hem', 'All'],\n",
        "                            title = \"Confusion Matrix without Normalization\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# img2 = plot_confusion_matrix(cm = np.array([[2808,  192],\n",
        "#                                             [227,  2773]]), \n",
        "#                             normalize = True,\n",
        "#                             target_names = ['all', 'hem'],\n",
        "#                             title = \"Confusion Matrix with Normalization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLRVP4hj6xPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}